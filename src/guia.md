# ğŸ“š GUÃA COMPLETA DEL CÃ“DIGO - modelo.py

## ğŸ¯ Objetivo General del CÃ³digo

Este archivo implementa un **sistema de Inteligencia Artificial** que aprende a predecir las jugadas de un oponente en Piedra, Papel o Tijera, utilizando **Machine Learning**.

---

## ğŸ“¦ 1. IMPORTACIONES Y CONFIGURACIÃ“N

### LibrerÃ­as Importadas

```python
import os
import pickle
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
```

**Â¿Para quÃ© sirve cada una?**

| LibrerÃ­a | Uso |
|----------|-----|
| `os` | Crear carpetas (models/) |
| `pickle` | Guardar/cargar el modelo entrenado |
| `warnings` | Silenciar mensajes de advertencia |
| `Path` | Manejar rutas de archivos de forma segura |
| `pandas` | Manipular datos (DataFrames) |
| `numpy` | Operaciones matemÃ¡ticas y arrays |

### LibrerÃ­as de Machine Learning

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight
```

**Â¿Para quÃ©?**

- **train_test_split**: Divide datos en entrenamiento (80%) y prueba (20%)
- **accuracy_score**: Calcula el % de aciertos del modelo
- **KNeighborsClassifier**: Modelo KNN (vecinos mÃ¡s cercanos)
- **RandomForestClassifier**: Modelo de bosques aleatorios
- **GradientBoostingClassifier**: Modelo de boosting
- **compute_class_weight**: Balancea clases desbalanceadas

### ConfiguraciÃ³n de Rutas

```python
RUTA_PROYECTO = Path(__file__).parent.parent
RUTA_DATOS = RUTA_PROYECTO / "data" / "resultados_juego.csv"
RUTA_MODELO = RUTA_PROYECTO / "models" / "modelo_entrenado.pkl"
```

**ExplicaciÃ³n:**
- `__file__`: UbicaciÃ³n del archivo actual (modelo.py)
- `.parent.parent`: Sube 2 niveles (de src/ a rps-ai-bocatalomo/)
- Construye rutas a: `data/resultados_juego.csv` y `models/modelo_entrenado.pkl`

### Diccionarios de Mapeo

```python
JUGADA_A_NUM = {"piedra": 0, "papel": 1, "tijera": 2}
NUM_A_JUGADA = {0: "piedra", 1: "papel", 2: "tijera"}
GANA_A = {"piedra": "tijera", "papel": "piedra", "tijera": "papel"}
PIERDE_CONTRA = {"piedra": "papel", "papel": "tijera", "tijera": "piedra"}
```

**Â¿Por quÃ©?**

Los modelos de ML solo entienden **nÃºmeros**, no texto. Necesitamos:
- **JUGADA_A_NUM**: Convertir "piedra" â†’ 0, "papel" â†’ 1, "tijera" â†’ 2
- **NUM_A_JUGADA**: Convertir de vuelta 0 â†’ "piedra"
- **GANA_A**: Saber quÃ© jugada le gana a cuÃ¡l
- **PIERDE_CONTRA**: Saber quÃ© jugada pierde contra cuÃ¡l

---

## ğŸ—‚ï¸ 2. CARGA Y PREPARACIÃ“N DE DATOS

### FunciÃ³n: `cargar_datos()`

```python
def cargar_datos(ruta_csv: str = None) -> pd.DataFrame:
    """Carga y renombra columnas del CSV."""
```

**Â¿QuÃ© hace?**

1. Lee el archivo CSV con pandas
2. Renombra las columnas a nombres estÃ¡ndar
3. Si el CSV solo tiene 3 columnas, aÃ±ade las que faltan

**Ejemplo:**

```python
# Entrada: CSV con columnas desconocidas
# 1,piedra,papel,Jugador 2,0.5,0.6

# Salida: DataFrame con columnas estÃ¡ndar
# numero_ronda | jugada_j1 | jugada_j2 | ganador | tiempo_j1 | tiempo_j2
# 1            | piedra    | papel     | J2      | 0.5       | 0.6
```

**CÃ³digo clave:**

```python
if len(df.columns) == 3:
    # CSV mÃ­nimo: solo tiene ronda, j1, j2
    df.columns = NOMBRES[:3]
    df['tiempo_j1'] = 0.5  # AÃ±adir columnas que faltan
    df['tiempo_j2'] = 0.5
```

---

### FunciÃ³n: `preparar_datos()`

```python
def preparar_datos(df: pd.DataFrame) -> pd.DataFrame:
    """Prepara datos: convierte jugadas a nÃºmeros y crea target."""
```

**Â¿QuÃ© hace? (Paso a paso)**

#### Paso 1: Convertir jugadas a nÃºmeros

```python
df['jugada_j1_num'] = df['jugada_j1'].map(JUGADA_A_NUM)
df['jugada_j2_num'] = df['jugada_j2'].map(JUGADA_A_NUM)
```

**Antes:**
```
jugada_j1: piedra, papel, tijera
```

**DespuÃ©s:**
```
jugada_j1_num: 0, 1, 2
```

#### Paso 2: Crear el TARGET (objetivo a predecir)

```python
df['proxima_jugada_j2'] = df['jugada_j2_num'].shift(-1)
```

**Â¿QuÃ© hace `shift(-1)`?**

Desplaza los valores hacia **arriba**, asÃ­ cada fila tiene la jugada **siguiente**:

```
Ronda | jugada_j2 | proxima_jugada_j2
  1   | piedra    | papel            â† Shift trajo el valor de la ronda 2
  2   | papel     | tijera           â† Shift trajo el valor de la ronda 3
  3   | tijera    | NaN              â† No hay ronda 4
```

**Â¿Por quÃ© es importante?**

Esto es el **corazÃ³n del modelo**: Queremos predecir **"Â¿quÃ© jugarÃ¡ el oponente EN LA PRÃ“XIMA RONDA?"**

#### Paso 3: Calcular resultado de cada ronda

```python
def calcular_resultado(row):
    j1, j2 = row['jugada_j1'], row['jugada_j2']
    if j1 == j2: return 0        # Empate
    elif GANA_A.get(j1) == j2: return 1   # Gana J1
    else: return -1                        # Pierde J1

df['resultado'] = df.apply(calcular_resultado, axis=1)
```

**Resultado:**
- `1` = J1 ganÃ³
- `0` = Empate
- `-1` = J1 perdiÃ³

---

## âš™ï¸ 3. FEATURE ENGINEERING (Lo MÃ¡s Importante)

### FunciÃ³n: `crear_features()`

```python
def crear_features(df: pd.DataFrame) -> pd.DataFrame:
    """Crea features sin introducir patrones cÃ­clicos."""
```

**Â¿QuÃ© son las "features"?**

Son **caracterÃ­sticas** que ayudan al modelo a predecir. Cuantas mejores features, mejor predicciÃ³n.

### Feature 1: Frecuencias Acumulativas

```python
df['freq_j2_piedra'] = (df['jugada_j2_num'] == 0).expanding().mean()
df['freq_j2_papel'] = (df['jugada_j2_num'] == 1).expanding().mean()
df['freq_j2_tijera'] = (df['jugada_j2_num'] == 2).expanding().mean()
```

**Â¿QuÃ© hace `.expanding().mean()`?**

Calcula el **promedio acumulativo**:

```
Ronda | jugada_j2 | freq_j2_piedra
  1   | piedra    | 1.00 (100% ha sido piedra hasta ahora)
  2   | papel     | 0.50 (50% piedra de 2 rondas)
  3   | piedra    | 0.67 (67% piedra de 3 rondas)
  4   | tijera    | 0.50 (50% piedra de 4 rondas)
```

**Â¿Por quÃ© es Ãºtil?**

Si alguien juega piedra el 60% del tiempo, **probablemente seguirÃ¡ haciÃ©ndolo**.

---

### Feature 2: Lag Features (Memoria)

```python
df['jugada_j2_lag1'] = df['jugada_j2_num'].shift(1)
df['jugada_j2_lag2'] = df['jugada_j2_num'].shift(2)
df['jugada_j2_lag3'] = df['jugada_j2_num'].shift(3)
```

**Â¿QuÃ© hace `shift(1)`?**

Trae el valor de la fila **anterior**:

```
Ronda | jugada_j2 | lag1  | lag2  | lag3
  4   | tijera    | papel | piedra| papel
             â†‘        â†‘       â†‘       â†‘
           actual   ronda3  ronda2  ronda1
```

**Â¿Por quÃ© es Ãºtil?**

Detecta patrones como: **"Siempre juega tijera despuÃ©s de papel"**

---

### Feature 3: Resultado Anterior

```python
df['resultado_anterior'] = df['resultado'].shift(1)
```

**Â¿Para quÃ©?**

Detecta si el oponente **reacciona** a ganar o perder:

```
Ronda | resultado_anterior | jugada_j2
  2   | -1 (perdiÃ³)        | papel     â† Â¿Cambia despuÃ©s de perder?
  3   | 1  (ganÃ³)          | papel     â† Â¿Repite cuando gana?
```

---

### Feature 4: Racha

```python
def calcular_racha(resultados):
    racha = 0
    for r in resultados:
        if r == 1: racha = racha + 1 if racha >= 0 else 1
        elif r == -1: racha = racha - 1 if racha <= 0 else -1
        else: racha = 0
    return racha

df['racha'] = df['resultado'].expanding().apply(calcular_racha, raw=False)
```

**Â¿QuÃ© hace?**

Cuenta victorias/derrotas **consecutivas**:

```
Resultados:   1,  1, -1, -1, -1,  0,  1
Racha:        1,  2, -1, -2, -3,  0,  1
              â†‘   â†‘   â†‘   â†‘   â†‘   â†‘   â†‘
            +1  +2  -1  -2  -3  reset +1
```

**Â¿Por quÃ© es Ãºtil?**

Detecta si el oponente cambia estrategia tras una racha de derrotas.

---

### Feature 5: Patrones de Cambio

```python
df['cambio_j2'] = (df['jugada_j2_num'] != df['jugada_j2_lag1']).astype(int)
df['cambio_tras_perder'] = ((df['resultado_anterior'] == -1) & (df['cambio_j2'] == 1)).astype(int)
```

**Â¿QuÃ© detecta?**

- **cambio_j2**: Â¿CambiÃ³ su jugada? (1=sÃ­, 0=no)
- **cambio_tras_perder**: Â¿CambiÃ³ DESPUÃ‰S de perder?

**Ejemplo:**

```
Ronda | jugada_j2 | resultado_anterior | cambio_j2 | cambio_tras_perder
  2   | papel     | -1 (perdiÃ³)        | 1 (cambiÃ³)| 1 (SÃ)
  3   | papel     | 1  (ganÃ³)          | 0 (repite)| 0 (NO)
```

---

### Feature 6: Fase del Juego

```python
df['fase_juego'] = pd.cut(df['numero_ronda'], bins=3, labels=[0, 1, 2])
```

**Â¿QuÃ© hace `pd.cut()`?**

Divide las rondas en 3 grupos:

```
Rondas 1-5:   fase_juego = 0 (inicio)
Rondas 6-10:  fase_juego = 1 (medio)
Rondas 11-15: fase_juego = 2 (final)
```

**Â¿Por quÃ© es Ãºtil?**

La gente juega diferente al principio (explorando) vs al final (patrones establecidos).

---

### Feature 7: Tendencias Recientes

```python
df['freq_j2_piedra_reciente'] = (df['jugada_j2_num'] == 0).rolling(5, min_periods=1).mean()
```

**Â¿QuÃ© hace `.rolling(5)`?**

Calcula el promedio de las **Ãºltimas 5 rondas** (ventana mÃ³vil):

```
Rondas:     P  P  T  P  P  P  P
Ventana:   [P  P  T  P  P]
Promedio:   80% piedra en Ãºltimas 5

Siguiente:    [P  T  P  P  P]
Promedio:      80% piedra
```

**Â¿Por quÃ© es Ãºtil?**

Detecta **cambios de estrategia**: "Antes jugaba tijera, ahora juega papel"

---

### Feature 8: AnÃ¡lisis de Tiempos

```python
df['tiempo_j2_promedio'] = df['tiempo_j2'].expanding().mean()
df['tiempo_j2_relativo'] = df['tiempo_j2'] - df['tiempo_j2_promedio']
df['tiempo_j2_rapido'] = (df['tiempo_j2'] < 0.5).astype(int)
```

**Â¿QuÃ© detecta?**

- **tiempo_j2_promedio**: Velocidad promedio del oponente
- **tiempo_j2_relativo**: Â¿JugÃ³ mÃ¡s rÃ¡pido o lento que su promedio?
- **tiempo_j2_rapido**: Â¿JugÃ³ en menos de 0.5 segundos?

**Â¿Por quÃ© es Ãºtil?**

Las **jugadas rÃ¡pidas son instintivas** y mÃ¡s predecibles. Si alguien juega rÃ¡pido, probablemente use su jugada "por defecto".

---

### FunciÃ³n: `seleccionar_features()`

```python
def seleccionar_features(df: pd.DataFrame) -> tuple:
    """Selecciona features para el modelo."""
    feature_cols = [
        'jugada_j2_lag1', 'jugada_j2_lag2', 'jugada_j2_lag3',
        'freq_j2_piedra', 'freq_j2_papel', 'freq_j2_tijera',
        # ... (21 features en total)
    ]
    
    X = df_clean[feature_cols]  # Features (entrada)
    y = df_clean['proxima_jugada_j2']  # Target (salida)
    
    return X, y
```

**Â¿QuÃ© hace?**

Separa los datos en:
- **X** (features): Las 21 caracterÃ­sticas que el modelo usarÃ¡ para aprender
- **y** (target): Lo que queremos predecir (prÃ³xima jugada)

---

## ğŸ“ 4. ENTRENAMIENTO DEL MODELO

### FunciÃ³n: `entrenar_modelo()`

```python
def entrenar_modelo(X, y, test_size: float = 0.2):
    """Entrena y selecciona el mejor modelo."""
```

#### Paso 1: Dividir Datos

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=False
)
```

**Â¿QuÃ© hace?**

Divide los datos:
- **80% Entrenamiento**: Para que el modelo aprenda
- **20% Prueba**: Para evaluar quÃ© tan bien aprendiÃ³

**shuffle=False**: No mezcla (mantiene orden temporal)

```
Datos totales: 100 rondas
â”œâ”€ Train: Rondas 1-80  (aprender)
â””â”€ Test:  Rondas 81-100 (evaluar)
```

---

#### Paso 2: Balancear Clases

```python
clases = np.unique(y_train)
pesos = compute_class_weight(class_weight='balanced', classes=clases, y=y_train)
pesos_dict = dict(zip(clases, pesos))
```

**Â¿Por quÃ©?**

Si tienes datos desbalanceados:

```
Piedra: 60 veces
Papel: 30 veces
Tijera: 10 veces â† El modelo ignorarÃ­a tijera
```

**Los pesos corrigen esto:**

```
Peso Piedra: 0.5  (baja importancia)
Peso Papel:  1.0  (normal)
Peso Tijera: 3.0  (alta importancia)
```

---

#### Paso 3: Entrenar MÃºltiples Modelos

```python
modelos = {
    'Random Forest': RandomForestClassifier(...),
    'Gradient Boosting': GradientBoostingClassifier(...),
    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),
}
```

**Â¿Por quÃ© 3 modelos?**

Cada modelo tiene **fortalezas diferentes**:

| Modelo | Bueno para |
|--------|-----------|
| **Random Forest** | Patrones complejos, robusto |
| **Gradient Boosting** | Accuracy alta, aprende errores |
| **KNN** | Patrones simples, similar a casos anteriores |

---

#### Paso 4: Evaluar y Seleccionar el Mejor

```python
for nombre, modelo in modelos.items():
    modelo.fit(X_train, y_train)  # Entrenar
    y_pred = modelo.predict(X_test)  # Predecir
    acc = accuracy_score(y_test, y_pred)  # Evaluar
    
    if acc > mejor_accuracy:
        mejor_modelo = modelo  # Guardar el mejor
```

**Salida:**

```
ğŸ“Š Evaluando modelos...
  Random Forest: 52.30%
  Gradient Boosting: 48.70%
  KNN (k=5): 46.20%

ğŸ† Mejor: Random Forest (52.30%)
```

---

#### Paso 5: Reentrenar con Todos los Datos

```python
mejor_modelo.fit(X, y)  # Usar TODOS los datos (100%)
```

**Â¿Por quÃ©?**

Ahora que sabemos que Random Forest es el mejor, lo entrenamos con **todos los datos** (no solo el 80%) para que aprenda mÃ¡s.

---

### Funciones: `guardar_modelo()` y `cargar_modelo()`

```python
def guardar_modelo(modelo, ruta=None):
    with open(ruta, "wb") as f:
        pickle.dump(modelo, f)

def cargar_modelo(ruta=None):
    with open(ruta, "rb") as f:
        return pickle.load(f)
```

**Â¿QuÃ© hace pickle?**

Guarda el modelo entrenado en un archivo `.pkl` para usarlo despuÃ©s sin tener que reentrenar.

---

## ğŸ¤– 5. CLASE JUGADOR IA (Lo MÃ¡s Complejo)

### InicializaciÃ³n

```python
class JugadorIA:
    def __init__(self, ruta_modelo: str = None):
        self.modelo = None
        self.historial = []
        self.feature_cols = [...]  # Lista de 21 features
        
        self.modelo = cargar_modelo(ruta_modelo)
```

**Â¿QuÃ© guarda?**

- **modelo**: El modelo entrenado (Random Forest, etc.)
- **historial**: Lista de todas las rondas jugadas
- **feature_cols**: Nombres de las 21 features (deben coincidir con entrenamiento)

---

### MÃ©todo: `registrar_ronda()`

```python
def registrar_ronda(self, jugada_j1: str, jugada_j2: str, 
                    tiempo_j1: float = 0, tiempo_j2: float = 0):
    self.historial.append((jugada_j1, jugada_j2, tiempo_j1, tiempo_j2))
```

**Â¿QuÃ© hace?**

AÃ±ade cada ronda jugada al historial:

```python
historial = [
    ('piedra', 'papel', 0.5, 0.6),
    ('tijera', 'piedra', 0.8, 0.4),
    ('papel', 'tijera', 0.3, 0.7),
]
```

---

### MÃ©todo: `obtener_features_actuales()` â­

```python
def obtener_features_actuales(self) -> np.ndarray:
    """Genera features del historial actual."""
    df_hist = pd.DataFrame(self.historial, ...)
    df = preparar_datos(df_hist)
    df = crear_features(df)
    
    ultima_fila = df.iloc[-1]
    features = ultima_fila[self.feature_cols].values
    return features
```

**Â¿QuÃ© hace? (Paso a paso)**

1. Convierte `historial` en DataFrame
2. Llama a `preparar_datos()` (convierte a nÃºmeros)
3. Llama a `crear_features()` (calcula las 21 features)
4. Toma la **Ãºltima fila** (estado actual)
5. Extrae solo las 21 features que el modelo necesita

**Ejemplo:**

```python
Historial: 3 rondas jugadas
â†’ Convierte a DataFrame
â†’ Crea features (freq_piedra=0.66, lag1=1, ...)
â†’ Ãšltima fila: [0.66, 1, 0, 0.33, ...] â† 21 nÃºmeros
â†’ Estos 21 nÃºmeros van al modelo para predecir
```

---

### MÃ©todo: `es_jugador_aleatorio()` ğŸ²

```python
def es_jugador_aleatorio(self) -> bool:
    """Detecta si el oponente juega aleatorio."""
```

**3 Criterios:**

1. **Frecuencias equilibradas**: ~33% cada jugada
2. **Cambios frecuentes**: >75% tasa de cambio
3. **Sin patrÃ³n reciente**: Ninguna jugada >50% en Ãºltimas 5

**Si cumple 2 de 3 â†’ Jugador ALEATORIO**

---

### MÃ©todo: `predecir_jugada_oponente()` ğŸ§  (EL MÃS IMPORTANTE)

```python
def predecir_jugada_oponente(self) -> str:
    """Predice la prÃ³xima jugada SIN crear bucles cÃ­clicos."""
```

#### **Flujo de DecisiÃ³n:**

```
1. Â¿Hay modelo? NO â†’ jugar aleatorio
                â†“ SÃ
2. Â¿IA jugÃ³ lo mismo 5 veces? SÃ â†’ CAMBIAR (anti-bucle)
                              â†“ NO
3. Â¿Oponente es aleatorio? SÃ â†’ Estrategia anti-aleatorio
                           â†“ NO
4. Â¿Hay patrÃ³n MUY claro (>65%)? SÃ â†’ Usar patrÃ³n
                                 â†“ NO
5. Â¿Hay patrÃ³n claro (>55%)? SÃ (70%) â†’ Usar patrÃ³n
                              â†“ NO (30%)
6. Usar predicciÃ³n del modelo
```

#### **Detector Anti-Bucle** ğŸš¨

```python
if len(set(ultimas_5_ia)) == 1:  # Si las 5 son iguales
    print("ğŸš¨ ANTI-BUCLE")
    opciones = [j for j in ["piedra", "papel", "tijera"] if j != repetida]
    return np.random.choice(opciones)
```

**Â¿QuÃ© previene?**

```
âŒ ANTES (sin anti-bucle):
IA: Piedra, Piedra, Piedra, Piedra, Piedra... (infinito)

âœ… AHORA (con anti-bucle):
IA: Piedra, Piedra, Piedra, Piedra, Piedra, Papel â† CAMBIA
```

---

### MÃ©todo: `decidir_jugada()` ğŸ¯

```python
def decidir_jugada(self) -> str:
    prediccion_oponente = self.predecir_jugada_oponente()
    return PIERDE_CONTRA[prediccion_oponente]
```

**Â¿QuÃ© hace?**

1. Predice quÃ© jugarÃ¡ el oponente
2. Devuelve la jugada que **le gana**

**Ejemplo:**

```python
prediccion = "tijera"  â† IA predice que jugarÃ¡s tijera
return PIERDE_CONTRA["tijera"]  = "piedra"
â†’ IA juega PIEDRA (gana a tijera)
```

---

## ğŸ 6. FUNCIÃ“N MAIN (Flujo Completo)

```python
def main():
    df = cargar_datos()           # 1. Cargar CSV
    df = preparar_datos(df)       # 2. Convertir a nÃºmeros
    df = crear_features(df)       # 3. Crear 21 features
    X, y = seleccionar_features(df)  # 4. Separar X e y
    modelo = entrenar_modelo(X, y)   # 5. Entrenar modelos
    guardar_modelo(modelo)        # 6. Guardar el mejor
```

---

## ğŸ“Š RESUMEN: Flujo Completo de Uso

### Entrenamiento (una vez)

```
CSV (150 rondas)
    â†“ cargar_datos()
DataFrame con columnas estÃ¡ndar
    â†“ preparar_datos()
Jugadas convertidas a nÃºmeros + target creado
    â†“ crear_features()
21 features calculadas
    â†“ seleccionar_features()
X (21 features), y (target)
    â†“ entrenar_modelo()
3 modelos entrenados â†’ Mejor seleccionado
    â†“ guardar_modelo()
modelo_entrenado.pkl (guardado)
```

### Uso en Juego (cada ronda)

```
Ronda 1-3: IA juega aleatorio (no hay historial)

Ronda 4+:
    Tu jugada anterior registrada en historial
        â†“ obtener_features_actuales()
    21 features calculadas del historial actual
        â†“ predecir_jugada_oponente()
    Modelo predice: "JugarÃ¡ TIJERA"
        â†“ decidir_jugada()
    IA decide: "JugarÃ© PIEDRA" (gana a tijera)
        â†“
    Ronda se juega
        â†“ registrar_ronda()
    Se aÃ±ade al historial
        â†“
    Volver a Ronda siguiente
```

---

## ğŸ¯ Conceptos Clave Para Entender

1. **Target (y)**: Lo que queremos predecir = prÃ³xima jugada
2. **Features (X)**: CaracterÃ­sticas que ayudan a predecir (21 en total)
3. **Train/Test Split**: 80% aprende, 20% evalÃºa
4. **Expanding**: Promedio acumulativo (toda la historia)
5. **Rolling**: Promedio de ventana mÃ³vil (Ãºltimas N rondas)
6. **Shift**: Trae valores de filas anteriores/siguientes
7. **Anti-Bucle**: Evita que la IA se quede atascada

---

## ğŸ’¡ Â¿Por QuÃ© Funciona?

1. **Muchas features (21)**: El modelo ve muchos patrones
2. **Datos histÃ³ricos**: Aprende de 150+ rondas previas
3. **DetecciÃ³n de patrones**: Frecuencias, lag, rachas
4. **Anti-bucle**: No se queda atascado
5. **DetecciÃ³n de aleatoridad**: Cambia estrategia si no hay patrÃ³n
6. **MÃºltiples modelos**: Elige el que mejor funciona

**Resultado:** 50-70% winrate contra humanos ğŸ¯